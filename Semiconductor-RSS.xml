<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Semiconductor Recap News</title>
    <link>https://github.com/EdsonBellido/semiconductor-RSS</link>
    <description>Articles for semiconductor professionals</description>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Dec 2025 09:01:18 GMT</lastBuildDate>
    <atom:link rel="hub" href="http://pubsubhubbub.appspot.com/" />
    <atom:link href="https://raw.githack.com/EdsonBellido/semiconductor-RSS/main/Semiconductor-RSS.xml" rel="self" type="application/rss+xml" />

    <item>
      <title>Tracing The Equipment Connectivity Journey</title>
      <link>https://semiengineering.com/tracing-the-equipment-connectivity-journey/</link>
      <description>&lt;p&gt;&lt;img src=&quot;https://semiengineering.com/wp-content/uploads/2025/10/AdobeStock_743839364-10-7-25-scaled.jpeg?fit=2560%2C1435&amp;amp;ssl=1&quot; alt=&quot;AdobeStock_743839364-10-7-25-scaled.jpeg&quot;&gt;&lt;/p&gt;&lt;p&gt;The semiconductor industry has undergone a dramatic transformation from the early days of manual integration to today’s AI-driven collaboration with equipment connectivity at the heart of this evolution. Understanding these trends is crucial for any organization looking to leverage data for improved efficiency, reliability, and competitive advantage.&lt;/p&gt;  
&lt;p&gt;This blog explores milestones and emerging trends in equipment connectivity, drawing on decades of industry experience, tracing the journey from proprietary interfaces to standardized protocols and the pivotal role of 300mm wafer automation. We will examine how cloud technologies, AI, and secure remote connectivity are shaping the future of manufacturing, creating new opportunities for collaboration and optimization across the entire supply chain. By understanding this evolution, businesses can better position themselves to harness the power of connected equipment and drive their own digital transformation.&lt;/p&gt;  
&lt;h2&gt;The early days: Custom integration and the dawn of standards&lt;/h2&gt;  
&lt;p&gt;In the early 1980s, the concept of equipment connectivity was in its infancy. Integrating factory automation systems such as robotics required extensive custom work. With no established standards for connecting to equipment, every remote start/stop function or material sensor required a bespoke solution. This approach was expensive, time-consuming and difficult to scale.&lt;/p&gt;  
&lt;p&gt;The introduction of the Semiconductor Equipment Communication Standard (SECS) marked a step forward. However, it was far from a “plug-and-play” solution. Implementations varied widely between equipment, making integration unreliable and costly. While it was possible to demonstrate successful automation, the lack of standardization made it impractical for widespread factory adoption.&lt;/p&gt;  
&lt;p&gt;A significant breakthrough came in the late 1980s with the Generic Equipment Model (GEM). The vision behind GEM was revolutionary: a single, standard interface for any factory to communicate with any piece of equipment. Early adoption was encouraged through initiatives by organizations such as SEMATECH, which funded equipment manufacturers to implement the standard. Despite these efforts, widespread adoption was slow as fab managers remained risk-averse.&lt;/p&gt;  
&lt;h2&gt;The 300mm transition: Automation becomes a necessity&lt;/h2&gt;  
&lt;p&gt;The turning point for equipment connectivity arrived around 2000 with the industry’s transition to 300mm wafers. The sheer size and weight of these wafers made manual handling impractical and unsafe. This necessitated a move to full factory automation, which in turn made a robust equipment interface non-negotiable.&lt;/p&gt;  
&lt;p&gt;The GEM standard was expanded to include GEM300 capabilities, specifically designed for 300mm automated fabs. Suddenly, reliable connectivity was not just a “nice-to-have” but a critical requirement for factory operations.&lt;/p&gt;  
&lt;p&gt;This shift created a significant opportunity for software providers that could deliver high-quality, reliable solutions. Companies began to focus on developing standardized software that equipment makers could integrate into their tools, allowing them to concentrate on their core process technologies. This focus on software quality and standardization ensured that equipment could operate reliably in any factory worldwide, whether in the US, Europe, or Asia.&lt;/p&gt;  
&lt;p&gt;Alongside GEM and GEM300, the Equipment Data Acquisition (EDA) standard emerged, providing a more flexible and powerful way to collect vast amounts of data from semiconductor equipment. These standards form the bedrock of modern equipment connectivity.&lt;/p&gt;  
&lt;h2&gt;The rise of cloud, AI, and secure connectivity&lt;/h2&gt;  
&lt;p&gt;Around 2015, another major trend began to take shape: the convergence of cloud computing, AI, and the Industrial Internet of Things (IIoT). The focus shifted from simple connectivity to creating “smart, connected” equipment. This vision involved moving away from traditional PC-based factory integration toward a more resilient data center model, similar to those used by tech giants like Google and Meta. This model, built on Linux, microservices architecture, and containers, offers unparalleled uptime and the ability to make incremental updates without system downtime.&lt;/p&gt;  
&lt;p&gt;Realizing this vision required a new level of data infrastructure. To apply AI effectively, companies need access to large volumes of data from their entire fleet of equipment.&lt;/p&gt;  
&lt;p&gt;This presented a challenge: how to securely extract and transfer massive datasets from manufacturing facilities around the world, giving rise to secure remote connectivity platforms. These platforms provide a critical link, allowing large equipment manufacturers to connect to their equipment fleets, transfer data back to central servers, and apply analytics and AI to improve equipment productivity. This capability has enabled a new business model centered on enhanced service contracts and recurring revenue streams.&lt;/p&gt;  
&lt;p&gt;The benefits are twofold:&lt;/p&gt;  
&lt;ul&gt;  
&lt;li&gt;&lt;strong&gt;For Equipment Makers:&lt;/strong&gt; Access to fleet data allows them to learn about equipment performance in real-world conditions, predict failures, and optimize maintenance schedules. They can offer premium service packages that guarantee higher uptime and reliability, strengthening their customer relationships.&lt;/li&gt;  
&lt;li&gt;&lt;strong&gt;For Fabs:&lt;/strong&gt; Secure remote access allows equipment experts to diagnose problems quickly, transfer log files, and get equipment back online with minimal downtime, significantly improving operational efficiency and productivity.&lt;/li&gt;  
&lt;/ul&gt;  
&lt;p&gt;Today, this secure data infrastructure is a foundational element for AI-driven collaboration. It enables the secure exchange of exabytes of data between fabs and Original Equipment Manufacturers (OEMs), creating a network for sharing insights and optimizing processes across the entire semiconductor ecosystem.&lt;/p&gt;  
&lt;h2&gt;The future: AI-driven collaboration and enterprise integration&lt;/h2&gt;  
&lt;p&gt;The next frontier in equipment connectivity is AI-driven collaboration. This involves collecting data and orchestrating it to automate and accelerate decisions. With a secure data infrastructure in place, AI agents can be deployed to act on data in real time.&lt;/p&gt;  
&lt;p&gt;This collaboration extends beyond the factory floor. Modern platforms can integrate manufacturing data with enterprise systems such as ERP. This allows for:&lt;/p&gt;  
&lt;ul&gt;  
&lt;li&gt;&lt;strong&gt;Real-time Business Insights&lt;/strong&gt; with the ability to derive accurate product costing based on actual resource consumption and get real-time updates on order status and yield.&lt;/li&gt;  
&lt;li&gt;&lt;strong&gt;Supply Chain Optimization &lt;/strong&gt;automating quality assurance processes and gaining real-time visibility into Work-in-Process (WIP) across the supply chain, including foundries, OSATs, and other external vendors.&lt;/li&gt;  
&lt;li&gt;&lt;strong&gt;AI-Ready Data&lt;/strong&gt; creates a common data model that aligns and contextualizes manufacturing data, making it “analytics-ready” for AI applications.&lt;/li&gt;  
&lt;/ul&gt;  
&lt;p&gt;Other industries inspire this principle. For example, Elon Musk has stated that Tesla’s most valuable asset is the data collected from its fleet of vehicles. The same holds true for manufacturing equipment. The ability to collect and analyze data from a fleet of tools is the key to predicting issues, optimizing performance, and delivering unprecedented value to customers.&lt;/p&gt;  
&lt;h2&gt;Charting your course&lt;/h2&gt;  
&lt;p&gt;The evolution of equipment connectivity from custom hacks to standardized, AI-driven platforms highlights a clear trend where data is the most valuable asset in modern manufacturing. As the volume of data exchanged across the industry continues to explode, the ability to securely connect to equipment, gather data, and apply intelligent analysis is no longer optional—it’s essential for survival and growth.&lt;/p&gt;  
&lt;p&gt;For organizations looking to thrive, the path forward is to embrace these trends. Invest in standardized connectivity solutions. Build a secure infrastructure for remote data access. And most important, develop the capabilities to turn that data into actionable insights through AI and advanced analytics. By doing so, you can unlock new levels of efficiency, drive innovation, and build a more resilient and collaborative enterprise.&lt;/p&gt;  
&lt;p&gt;The post &lt;a href=&quot;https://semiengineering.com/tracing-the-equipment-connectivity-journey/&quot;&gt;Tracing The Equipment Connectivity Journey&lt;/a&gt; appeared first on &lt;a href=&quot;https://semiengineering.com&quot;&gt;Semiconductor Engineering&lt;/a&gt;.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://semiengineering.com/tracing-the-equipment-connectivity-journey/</guid>
      <pubDate>Tue, 09 Dec 2025 08:37:47 GMT</pubDate>
    </item>

    <item>
      <title>Smaller Geometries, Bigger Demands: The Role Of OCD In GAA Logic And Vertical Gate DRAM Process Control</title>
      <link>https://semiengineering.com/smaller-geometries-bigger-demands-the-role-of-ocd-in-gaa-logic-and-vertical-gate-dram-process-control/</link>
      <description>&lt;p&gt;AI workloads are pushing the boundaries of compute, memory, and interconnect architectures, and to meet these goals, manufacturers are rapidly accelerating advanced logic and DRAM development. Chief among these innovations: gate-all-around (GAA) logic transistor and vertical gate (VG) DRAM, two device architectures that promise higher performance, improved power efficiency, and greater scalability.&lt;/p&gt; 
&lt;p&gt;However, the arrival of these device architectures introduces new levels of manufacturing complexity brought on by increasing high aspect ratios (HAR) and the continued shrinking of device dimensions by roughly 20–30% per generation. Fortunately, new capabilities in optical critical dimension (OCD) metrology are up to the challenge of measuring and monitoring features, such as gate profiles, film thickness, and structural uniformity, at the nanometer scale. With the right tools on hand, manufacturers can maintain high volume process control for GAA logic and VG DRAM in the AI era (Figure 1).&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://i0.wp.com/semiengineering.com/wp-content/uploads/2025/12/Onto_OCD-in-GAA-Logic-VG-DRAM-Process-Control-fig1.webp?resize=451%2C346&amp;amp;ssl=1&quot; alt=&quot;&quot; width=&quot;451&quot; height=&quot;346&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fig. 1: GAA (left) and vertical gate DRAM (right) present new challenges for process control.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;These new capabilities are enabling the delivery of accurate, high throughput measurements across complex 3D geometries like GAA and VG DRAM by offering smaller spot size, higher signal sensitivity, and enhanced precision.&lt;/p&gt; 
&lt;p&gt;But before we discuss these new capabilities, let’s dive deeper into the challenges of both device architectures. We’ll start with GAA.&lt;/p&gt; 
&lt;h2&gt;Challenges and solutions&lt;/h2&gt; 
&lt;p&gt;GAA transistors feature HAR channels and stacks of multi-layered nanosheets, in some cases as thin as 10nm. The manufacturing complexity lies in controlling individual nanowires and accurately characterizing buried nanowires within three-dimensional architectures. Precise control over nanowire dimensions is critical for achieving the desired electrical performance, including higher transistor speeds and lower power consumption. As such, GAA manufacturers need metrology solutions capable of extracting unique signals with high sensitivity and precision from each layer, even those deeply buried under semi-opaque stacks.&lt;/p&gt; 
&lt;p&gt;While GAA logic presents unique challenges in transistor scaling, VG DRAM introduces equally complex demands in memory architecture. VG DRAM involves vertical nanowire etch, multi-layer ultra-thin film deposition, sidewall trim, and buried bitlines, electrical connections that are embedded deep within the silicon substrate rather than being placed on the surface, helping reduce the footprint of each memory cell. Additionally, VG DRAM for HBM4 features smaller cell blocks. Addressing these challenges requires an OCD system with a significantly smaller spot size in order to conduct direct on-device measurements. This, unfortunately, reduces the signal strength of optical systems.&lt;/p&gt; 
&lt;p&gt;For both GAA logic and VG DRAM devices, incorporating real-time stabilization technologies and advanced optical modeling can improve measurement precision and enable tighter process windows. In addition, AI-guided analysis allows fabs to create robust measurement recipes faster and with greater accuracy for highly complex device structures like nanosheets and VG DRAM cells. Analysis software also enables more accurate measurements that are scalable across multiple tools, while supporting smart manufacturing and predictive analytics for advanced AI applications.&lt;/p&gt; 
&lt;p&gt;Many of today’s OCD metrology tools, however, have limited SNR to measure these increasingly smaller changes in the complex device structure, while other systems do not have sufficient information to separate signals from multiple parameters (e.g., dimensions from individual nanowires) in complex device structures. Furthermore, most OCD metrology systems feature large spot sizes that do not fit into the reduced DRAM cell block area for HBM4.&lt;/p&gt; 
&lt;p&gt;Without the proper advanced metrology solutions addressing these challenges, manufacturers risk variability in critical dimensions and compromising device performance and yield. As such, more data channels and enhanced SNR at higher speed are needed to overcome the reduced signal strength due to smaller geometries and increased signal complexity from higher nanowire stacks.&lt;/p&gt; 
&lt;p&gt;Onto Innovation has recently added a number of optical and algorithm innovations to its OCD arsenal, such as new multi-polarization angle data channels with real-time stabilization technologies to enhance information content and SNR. These new capabilities offer advanced optical modeling, improving measurement precision, and robustness. Furthermore, the addition of AI-guided analysis software allows fabs to create robust measurement recipes faster and with greater accuracy for highly complex device structures. This includes nanosheets and VG DRAM cells. The use of analytical software also enables more accurate measurements and is scalable across multiple tools, all in support of smart manufacturing and predictive analytics for advanced AI applications.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;As the semiconductor industry advances toward AI-optimized architectures, the need for more precise and adaptive process control has become critical. The &lt;a href=&quot;https://ontoinnovation.com/products/atlas-g6-system/&quot;&gt;new OCD capabilities&lt;/a&gt; discussed in this blog enable tighter control over individual nanowires in GAA logic and allow for more accurate measurements within increasingly compact VG DRAM cell blocks. Combined, these innovations represent a foundational shift in how the industry measures, monitors, and optimizes the most advanced semiconductor technologies.&lt;/p&gt; 
&lt;p&gt;The post &lt;a href=&quot;https://semiengineering.com/smaller-geometries-bigger-demands-the-role-of-ocd-in-gaa-logic-and-vertical-gate-dram-process-control/&quot;&gt;Smaller Geometries, Bigger Demands: The Role Of OCD In GAA Logic And Vertical Gate DRAM Process Control&lt;/a&gt; appeared first on &lt;a href=&quot;https://semiengineering.com&quot;&gt;Semiconductor Engineering&lt;/a&gt;.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://semiengineering.com/smaller-geometries-bigger-demands-the-role-of-ocd-in-gaa-logic-and-vertical-gate-dram-process-control/</guid>
      <pubDate>Tue, 09 Dec 2025 08:37:47 GMT</pubDate>
    </item>

    <item>
      <title>Adaptive Test Gaining Ground For HPC And AI Chips</title>
      <link>https://semiengineering.com/adaptive-test-gaining-ground-for-hpc-and-ai-chips/</link>
      <description>&lt;p&gt;&lt;img src=&quot;https://semiengineering.com/wp-content/uploads/2017/05/semi_fb_thumb.jpg?fit=250%2C250&amp;amp;ssl=1&quot; alt=&quot;semi_fb_thumb.jpg?fit=250%2C250&amp;amp;ssl=1&quot;&gt;&lt;/p&gt;&lt;p&gt;Adaptive test is starting to gain traction for high-performance computing and AI chips as test programs that rely on static limits and fixed test sequences reach their practical limits.&lt;/p&gt;  
&lt;p&gt;The growing complexity of multi-die assemblies and power delivery, along with increased stresses, are forcing a shift toward real-time, data-driven optimization at the test cell.&lt;/p&gt;  
&lt;p&gt;“It’s the same old problem,” said Brent Bullock, test technology director at &lt;a href=&quot;https://semiengineering.com/entities/advantest-corporation/&quot;&gt;Advantest&lt;/a&gt;. “If you don’t have the right data, all the intelligence in the world doesn’t help you. You’re just guessing.”&lt;/p&gt;  
&lt;p&gt;Adaptive test offers a way to adjust test conditions on the fly, predict failures before they occur, and focus expensive test resources where they matter most. Yet adopting these techniques in production is more complicated than just bolting machine learning onto a tester. Every component of the test environment must remain stable enough for models to function. The data pipeline must be accurate, timely, and complete, and the models themselves must be carefully validated to avoid costly escapes or unnecessary overkill.&lt;/p&gt;  
&lt;p&gt;&lt;strong&gt;Scaling test insight&lt;/strong&gt;&lt;br&gt;  
The growing reliance on real-time insight places new expectations on both the test infrastructure and the data that flows through it. Adaptive techniques can only be as effective as the measurements and models they depend on. The value is clear, but so are the challenges. Static testing may no longer be adequate, but dynamic testing introduces complexities that must be understood before they can be trusted.&lt;/p&gt;  
&lt;p&gt;The first problem is recognizing that much of what engineers want from adaptive test depends on data that is not readily accessible or simply does not exist. HPC and AI architectures are increasingly defined by their interactions across multiple voltage islands, chiplets, and localized thermal traps. Due to variation at those scales, it is extremely difficult to get consistent data. Even the same chip may produce different results depending on microenvironmental influences. Variation no longer follows clean statistical patterns across a wafer or lot. Instead, it often appears as localized shifts in timing or power behavior that emerge only under load. That makes test programs harder to optimize with traditional guardbands and predetermined sequences.&lt;/p&gt;  
&lt;p&gt;“You can’t fully mimic real workloads upfront,” said Alex Burlak, vice president of test and analytics at &lt;a href=&quot;https://semiengineering.com/entities/proteantecs/&quot;&gt;proteanTecs&lt;/a&gt;. “You need observability in the silicon so you can see what the device is doing while it’s operating.”&lt;/p&gt;  
&lt;p&gt;Modern devices also generate far more data than earlier-generation test systems were designed to handle. An HPC processor or AI accelerator can produce millions of data points across wafer sort, package test, and system-level evaluation. These measurements are often needed at multiple test insertions to enable predictive or adaptive decisions. Routing, synchronizing, and filtering that information fast enough to influence real-time test behavior has become a new engineering problem in its own right.&lt;/p&gt;  
&lt;p&gt;“There is a lot of value in big-data analytics, but if you want to make decisions for screening or optimization, you must deploy that on the tester,” said Burlak. “If it cannot operate in real time, you cannot use it in production.”&lt;/p&gt;  
&lt;p&gt;At the same time, engineers must confront the fact that the test cell itself introduces variability. Sockets, probe cards, load boards, and power delivery systems all add their own fingerprints to the measurement environment. When adaptive techniques tighten limits or flag marginal behavior, even small mechanical or electrical inconsistencies can influence the outcome. Stability, calibration, and continuous monitoring across the entire test stack are essential.&lt;/p&gt;  
&lt;p&gt;“The test socket or probe interconnect is often the forgotten part of the measurement path, it’s often assumed good and not a major variable,” said Jack Lewis, CTO of &lt;a href=&quot;https://semiengineering.com/entities/modus-test/&quot;&gt;Modus Test&lt;/a&gt;. “Test sockets and probe interconnects introduce mechanical variability into the measurement, which creates electrical variability, and that variability shows up in your results.”&lt;/p&gt;  
&lt;p&gt;The emphasis in test is no longer solely on measurement accuracy, but on the timeliness, completeness, and contextual clarity of the data itself. To support adaptive decisions, test platforms must treat data as an operational asset rather than a reporting artifact. That means integrating feature extraction, metadata alignment, and genealogy tracking throughout the manufacturing flow, often in ways that challenge legacy systems.&lt;/p&gt;  
&lt;p&gt;“Validation isn’t only about the final result. It starts with the data,” said Jin Yu, head of machine learning at &lt;a href=&quot;https://semiengineering.com/entities/teradyne-corporation/&quot;&gt;Teradyne&lt;/a&gt;. “We spend a lot of time verifying that the data quality is right before building models, because if you just throw everything together and hope for a good outcome, it won’t happen.”&lt;/p&gt;  
&lt;p&gt;The growing need for data orchestration is one of the main reasons adaptive test has not yet become standard practice. The techniques are promising, but the engineering foundation required to support them is still under construction in many organizations. For test teams that traditionally have relied on deterministic, procedure-based workflows, adopting a more fluid, data-driven model requires new investment in software and modeling, along with collaborative processes that span design, test, and manufacturing.&lt;/p&gt;  
&lt;p&gt;“It’s not just about looking at test results. You have to bring together the test data, the equipment data, the OEE data, and the prior process data so you can connect the dots,” said Aftkhar Aslam, CEO at &lt;a href=&quot;https://semiengineering.com/entities/yieldwerx/&quot;&gt;YieldWerx&lt;/a&gt;. “That’s what enables true feed-forward and feedback decisions, where AI and machine learning can recommend which actions to take and which ones to avoid.”&lt;/p&gt;  
&lt;p&gt;This also raises deeper questions about how engineers validate models that influence production decisions. Predictive screening and adaptive limits appeal to cost-sensitive organizations, but only when the analysis is transparent enough to gain trust. Engineers need to know why a model labels a device as marginal, and what specific features contribute to that classification. Without that clarity, skepticism remains high.&lt;/p&gt;  
&lt;p&gt;“It’s like Clarke’s third law,” said Marc Jacobs, senior director of solutions architecture at &lt;a href=&quot;https://semiengineering.com/entities/pdf-solutions/&quot;&gt;PDF Solutions&lt;/a&gt;. “People want something indistinguishable from magic to solve problems that regular engineering can’t. For example, they might want a machine-learning model for field failures, but the problem with machine learning is that you need significant numbers of field failures to train the model accurately, and nobody wants to be the company shipping that many failures.”&lt;/p&gt;  
&lt;p&gt;Furthermore, errors in adaptive classification can be expensive. Overly aggressive limits can eliminate good silicon, adding direct cost and supply risk. Insufficiently conservative limits can allow latent defects to escape, where they may appear unexpectedly in the field. Both outcomes undermine the value of adaptive techniques and highlight the need for careful validation.&lt;/p&gt;  
&lt;p&gt;“When people start deploying AI at scale on manufacturing data, there are often a lot of garbage-in, garbage-out situations,” said John Kibarian, president and CEO of PDF Solutions, in a recent presentation. “You only really start to understand what your true data quality is once you have context for that data.”&lt;/p&gt;  
&lt;p&gt;&lt;strong&gt;Adaptive adoption challenges&lt;/strong&gt;&lt;br&gt;  
Some companies are finding early success with predictive screening and dynamic limits, but others caution that mechanical variation, data quality, and model drift still limit broad adoption. Nearly everyone agrees that demand for adaptive test is rising, but most are skeptical that it will be a straightforward transition.&lt;/p&gt;  
&lt;p&gt;“Two approaches we see are hold-back or skip programs to validate changes, so you have a constant check that you’re not creating a product problem,” said Jacobs. “If the input data shifts drastically, then the model may not behave the way you expect.”&lt;/p&gt;  
&lt;p&gt;Early results are mixed because adaptive methods touch every part of the test stack. The wins tend to come where teams already have clean telemetry, disciplined limits management, and a way to execute decisions at the edge. The misses show up when model inputs drift faster than governance can respond, or when mechanical variation and noisy fixtures swamp small-signal effects. In actual practice, most organizations are still building the infrastructure, with feature pipelines between insertions, synchronized IDs across suppliers, and guardrails that keep models inside known-safe operating envelopes.&lt;/p&gt;  
&lt;p&gt;&lt;strong&gt;Building trustworthy signals&lt;/strong&gt;&lt;br&gt;  
The near-term playbook focuses on pragmatics. Start with use cases that are insensitive to small metrology noise, such as redundancy pruning and outlier screening on stable parameters. Instrument the test cell to separate device behavior from socket and power-path effects, and make edge execution a first-class requirement rather than a future enhancement. Treat models like production equipment that need calibration schedules, change control, and rollback plans. The goal is fewer brittle, monolithic models and more narrowly scoped ones that are easier to validate and retire when conditions change.&lt;/p&gt;  
&lt;p&gt;“Right now, the main thing people are doing is monitoring for drift and shift,” said Jacobs. “If someone makes a test-program change or fixturing moves, the model will drift. The first step is to debug the issue. If the probe card has some flux residue causing leakage, you don’t retrain your model.”&lt;/p&gt;  
&lt;p&gt;Adaptive test also hinges on explainability. Test teams need to know which features drove a decision and how those features map to physical failure modes, not just a test score.&lt;/p&gt;  
&lt;p&gt;“For a long time, when we handled the test data, we assumed the test instrument was perfect, but it’s not,” said Teradyne’s Yu. “The instrument data, calibration data, and service patterns all need to be considered. If the model only looks at the device side, it doesn’t see when the instrument itself needs attention.”&lt;/p&gt;  
&lt;p&gt;That transparency is what lets a product engineer decide whether to tighten a limit, route a lot to stress testing, or trigger a tool-side investigation. Without it, the safest choice is to do nothing, which defeats the purpose of adaptive control.&lt;/p&gt;  
&lt;p&gt;Even when organizations have the infrastructure to support adaptive methods, the fundamental challenge often comes down to signal quality. The parameters most useful for prediction are rarely the ones that are easiest to measure. Subtle timing margins, droop signatures, thermal interactions, and coupling effects can drift as the package settles or as the device experiences real workloads. Extracting meaningful signals requires instrumentation that can survive variation in the test cell, electrical loading, and environmental conditions.&lt;/p&gt;  
&lt;p&gt;“You need enough visibility into the device to know what is happening electrically,” said proteanTecs’ Burlak. “If you only see the final outcome, you cannot tell whether the part is changing or your environment is changing.”&lt;/p&gt;  
&lt;p&gt;Still, even with built-in observability, small shifts in external conditions can distort the measurements feeding an adaptive model. Thermal settling, probe-to-pad contact variation, and power-path impedance all influence how a device behaves under stress. These factors become more pronounced in HPC and AI devices that operate near thermal or electrical limits. If the environment shifts faster than the model can track it, previously stable features become unreliable.&lt;/p&gt;  
&lt;p&gt;“Latent defects aren’t discovered at time zero. It usually requires some kind of stress put on the device — high voltage or temperature over time — and burn-in serves that purpose,” said Davette Berry, senior director of customer programs and business development at Advantest. “Most commercial products have gotten away from doing burn-in, but most of these high-performance compute devices are having to put it back in to the product test flow, because having it fail six hours after it’s been installed in the data center is much worse than adding a burn-in test insertion.”&lt;/p&gt;  
&lt;p&gt;Once a clean signal is identified, the next challenge is aligning measurements across insertions. Many adaptive strategies rely on correlating wafer-sort behavior with package or inspection data, which requires stable identifiers and comparable conditions. Small mismatches between insertions can produce apparent trends that have nothing to do with silicon health. Engineers often discover inconsistencies not in the device, but in the metadata that accompanies it.&lt;/p&gt;  
&lt;p&gt;“In the ideal situation, you would measure every location, every die, every wafer but that’s not possible,” said Joe Kwan, director of product management at &lt;a href=&quot;https://semiengineering.com/entities/mentor-a-siemens-business/&quot;&gt;Siemens EDA&lt;/a&gt;. “In reality, the data is very sparse. What we do is take that sparse metrology, combine it with the design data and the process information, and put it into a digital twin. Then we can predict what the metrology would look like in all the other places we didn’t physically measure.”&lt;/p&gt;  
&lt;p&gt;By extending sparse measurements with modeled context, engineers gain a fuller view of silicon behavior while avoiding the cycle-time hit of expanded inspection. That broader context becomes essential as packaged devices exhibit new thermal and electrical sensitivities that do not always appear at wafer sort.&lt;/p&gt;  
&lt;p&gt;The importance of physical context extends to the test conditions. Adaptive decisions are only as good as the stability of the conditions they rely on. Temperature gradients, socket thermal impedance, and power-delivery transients can create differences between otherwise identical devices. For HPC parts that draw large, rapidly changing currents, the test cell must be instrumented to capture the true mechanical and electrical state at the moment of measurement.&lt;/p&gt;  
&lt;p&gt;The result is a growing recognition that adaptive test is not simply a data-science problem. It is an infrastructure problem, a measurement science problem, and a correlation problem all rolled into one. For organizations pursuing adaptive methods, the early wins tend to come from reducing noise, instrumenting the environment, and aligning data structures across the test flow. The more stable the signal, the more reliable the model, and the easier it becomes to introduce controlled adaptation without risking escapes or unnecessary yield loss.&lt;/p&gt;  
&lt;p&gt;&lt;strong&gt;Adaptive test as new discipline&lt;/strong&gt;&lt;br&gt;  
If adaptive test is going to scale beyond isolated use cases, the supporting infrastructure has to evolve from a deterministic, stepwise flow into something that can react in real-time. Traditional test assumes stability — stable limits, fixturing, data paths, and relationships between insertions. Adaptive methods challenge those assumptions by introducing feedback loops into places that historically only consumed data. That shift exposes weaknesses in handoffs, metadata, and change control that were less visible in fixed-sequence programs.&lt;/p&gt;  
&lt;p&gt;A bottleneck arises when test engineers attempt to combine device telemetry, environmental measurements, and upstream context into a single actionable model. Each signal reflects a slightly different view of the device, and unless those views are synchronized, the model will treat inconsistency as variation. Organizations accustomed to static guardbands may find that their first adaptive experiments fail because the infrastructure around the tester was never designed to maintain continuity across so many moving parts, not because the underlying science is flawed.&lt;/p&gt;  
&lt;p&gt;Adaptive test also alters who owns the decisions. Limits engineering, product engineering, data science, and manufacturing operations all have to treat test as a dynamic system rather than a static script. That requires new governance, calibrations, and explicit roles for validating models and monitoring drift. Companies that make progress with adaptive test tend to be the ones willing to unify disciplines that historically operated in parallel rather than together.&lt;/p&gt;  
&lt;p&gt;This is not primarily a modeling problem. It is a coordination problem. Production environments are full of micro-decisions that depend on accurate IDs, consistent metadata, synchronized timestamps, and stable supply-chain handoffs. Even a strong predictive model will fail if it receives misaligned features or conditions that drift faster than its update cycle.&lt;/p&gt;  
&lt;p&gt;“Collaboration isn’t something that happens only at the tail end,” said YieldWerx’s Aslam. “It runs through the entire product lifecycle from design and DFT, to building and evaluating test chips, to feeding those results back into simulation and verification. As you move into manufacturing, you’re working with product, package, test, and quality engineers, and even field teams once early samples reach applications. It’s a closed loop, and every stage influences the next.”&lt;/p&gt;  
&lt;p&gt;The move toward real-time control also forces teams to revisit how engineering work is divided. Historically, test engineers wrote patterns, debugged failures, and tuned limits, while manufacturing owned execution. Adaptive programs blur those boundaries. If a model tightens or relaxes limits at the edge, someone must define the allowed range of motion, certify that the signals feeding the model have not shifted, and decide when the model should be retrained, rolled back, or retired. None of those responsibilities map neatly to legacy roles.&lt;/p&gt;  
&lt;p&gt;What emerges is a picture of adaptive testing as both a technical and cultural transition. The technical work involves cleaning signals, synchronizing data, instrumenting the test cell, and validating models under drift. The cultural work involves redefining ownership and closing communication gaps between design, test, and fabrication. Treating adaptive test as an engineering discipline, rather than as a bolt-on feature, is what turns isolated wins into a sustainable capability.&lt;/p&gt;  
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;br&gt;  
Adaptive test is moving from theory to practice, but the transition is neither linear nor guaranteed. The promise is certainly compelling. Engineers want test systems that respond to real device behavior, allocate time where it matters most, and prevent downstream surprises in high-value HPC and AI products. Yet each step toward that future exposes another dependency. Clean telemetry, stable measurement conditions, synchronized identifiers, interpretable models, and edge execution all need to align before adaptation can safely influence production decisions.&lt;/p&gt;  
&lt;p&gt;The encouraging trend is that these capabilities are emerging together. Built-in observability is improving. Test cell instrumentation is becoming more precise. Analytics pipelines are maturing, and the organizations deploying them are learning what it actually takes to maintain model health over time. At the same time, engineers are developing a more practical understanding of when adaptive methods add value and when they introduce unnecessary risk. Most of the early successes come from removing noise, tightening correlations, and establishing the governance needed to trust even small adjustments, not from aggressive control.&lt;/p&gt;  
&lt;p&gt;The real shift is cultural as much as technical. Adaptive test requires teams to think of models the way they think of equipment. It requires organizations to treat data lineage as part of the manufacturing flow and to align design, test, and operations around shared signals rather than isolated metrics. That shift will take time, but the direction is clear. As complexity continues to rise, deterministic programs will struggle under the weight of their own assumptions, and dynamic strategies will become less an innovation than a necessity. The companies that succeed will be the ones that build stable signals first, apply adaptation cautiously, and treat explainability as a requirement rather than an aspiration.&lt;/p&gt;  
&lt;p&gt;The post &lt;a href=&quot;https://semiengineering.com/adaptive-test-gaining-ground-for-hpc-and-ai-chips/&quot;&gt;Adaptive Test Gaining Ground For HPC And AI Chips&lt;/a&gt; appeared first on &lt;a href=&quot;https://semiengineering.com&quot;&gt;Semiconductor Engineering&lt;/a&gt;.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://semiengineering.com/adaptive-test-gaining-ground-for-hpc-and-ai-chips/</guid>
      <pubDate>Tue, 09 Dec 2025 08:37:47 GMT</pubDate>
    </item>

    <item>
      <title>Invisible Interfaces: The Hidden Challenge Behind Every Great Image Sensor</title>
      <link>https://semiengineering.com/invisible-interfaces-the-hidden-challenge-behind-every-great-image-sensor/</link>
      <description>&lt;p&gt;When you snap a photo on your phone or rely on a car’s camera for lane detection, you’re trusting an unseen network of technologies to deliver or interpret image data flawlessly. But behind the scenes, the interface between the image sensor and its processor is doing the heavy lifting, moving megabtyes of data without error or delay.&lt;/p&gt; 
&lt;p&gt;While much of the industry conversation focuses on advances in resolution and sensor technology, another challenging aspect of modern imaging innovation is the interfaces—the invisible pathways that connect these sensors to the systems around them, including the processors tasked with interpreting their data. One of the most pressing and underappreciated imaging challenges lies in the ability of the interfaces to handle growing demands for speed, bandwidth, and reliability. The challenge isn’t one-size-fits-all. Smartphone cameras may need ultra-high resolution over short distances, while automotive sensors prioritize robustness and wider areas.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://i0.wp.com/semiengineering.com/wp-content/uploads/2025/12/Teradyne_Invisible-Interfaces-Challenge-Behind-Image-Sensor-fig1.webp?resize=1024%2C464&amp;amp;ssl=1&quot; alt=&quot;&quot; width=&quot;1024&quot; height=&quot;464&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Image sensor unit shipments by sub-segment. (Source: Yole &amp;amp; Teradyne)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;As image sensors and the technologies used to interpret the data evolve to deliver higher resolutions and even integrate artificial intelligence directly onto the chip, these interfaces are under more pressure than ever before. The challenge is both technical and practical: how do you design and test interfaces that must support vastly different applications, from the low-power demands of smartphones to the rugged, long-distance requirements of automotive systems?&lt;/p&gt; 
&lt;p&gt;And even more critically, how do you keep up when the rules change every few months?&lt;/p&gt; 
&lt;h2&gt;The growing challenge in image sensor development&lt;/h2&gt; 
&lt;p&gt;The industry’s insatiable appetite for higher resolutions is well known, but what often goes unnoticed is the corresponding explosion in data traffic. A single image sensor on a smartphone might capture 500 megabytes of data in one shot. In automotive systems, that sensor could be sending critical visual information across several meters of cabling to a centralized processor, where decisions like emergency braking or obstacle detection happen in real-time. Industrial imaging is pushing resolutions even higher (up to 500 megapixels in some cases) to support inspection and automation systems, creating enormous data handling and processing demands.&lt;/p&gt; 
&lt;p&gt;Each of these scenarios represents wildly different demands on the interfaces connecting sensors to the rest of the system. In smartphones, the processor is typically located just millimeters away from the image sensor. Power efficiency is paramount, and interfaces must support blisteringly fast data rates to process high-resolution images without draining the battery. In an automotive application, a vehicle’s safety system might require those same sensors to transmit data over longer distances and deliver real-time information and decision-making in harsh environments while meeting stringent reliability and safety standards.&lt;/p&gt; 
&lt;p&gt;It’s a challenge compounded by the fact that image sensor manufacturers rarely control these interface requirements. Industry-wide, sensor manufacturers are generally forced to adopt a growing variety of interface standards and proprietary solutions, each with unique requirements for bandwidth, distance, latency, and power consumption.&lt;/p&gt; 
&lt;p&gt;This creates a relentless cycle of adaptation, where manufacturers are forced to develop and validate new interfaces almost as quickly as they can design the sensors themselves. It’s not uncommon for entirely new interface requirements to be handed down with lead times as short as six months. Unpredictability follows for both image sensor designers and the teams responsible for testing these devices.&lt;/p&gt; 
&lt;h2&gt;The shift toward proprietary interfaces&lt;/h2&gt; 
&lt;p&gt;While MIPI remains the dominant open standard for image sensor interfaces, proprietary protocols are growing. These custom protocols are typically developed privately by major technology companies to support their unique product requirements, for example, to achieve specific performance advantages. These custom interfaces are closely guarded secrets and often remain entirely undocumented outside of the companies that develop them, making it extremely difficult for test equipment vendors to keep pace.&lt;/p&gt; 
&lt;p&gt;Even a full teardown of a high-end smartphone won’t reveal how its camera interfaces are engineered. Yet, despite having no access to these underlying specifications, test teams are still expected to validate sensor performance against them.&lt;/p&gt; 
&lt;p&gt;For manufacturers and test engineers, this creates a near-constant state of uncertainty. New protocols can emerge rapidly and without warning, and must be supported almost immediately, which can cause test equipment providers to scramble to retool systems.&lt;/p&gt; 
&lt;h2&gt;Flexibility as a strategic imperative&lt;/h2&gt; 
&lt;p&gt;Teradyne has set out to solve this challenge, developing a modular, future-ready approach that gives manufacturers the flexibility they need to thrive in unpredictable environments.&lt;/p&gt; 
&lt;p&gt;At the hardware level, Teradyne’s &lt;a href=&quot;https://www.teradyne.com/products/ultraflexplus/&quot;&gt;UltraSerial20G&lt;/a&gt; capture instrument for the &lt;a href=&quot;https://www.teradyne.com/products/ultraflexplus/&quot;&gt;UltraFLEXplus&lt;/a&gt; is designed for adaptability. Its modular architecture allows changes in key components and software to quickly accommodate new protocols.&lt;/p&gt; 
&lt;p&gt;Additional flexibility is added with Teradyne’s &lt;a href=&quot;https://www.teradyne.com/resources/ig-xl-software/&quot;&gt;IG-XL software&lt;/a&gt;. Customers are empowered to develop highly customized test strategies, controlling every detail of the testing process, from voltage and timing to signal slopes and data handling.&lt;/p&gt; 
&lt;h2&gt;The path ahead: Staying competitive in a fragmented, fast-moving market&lt;/h2&gt; 
&lt;p&gt;For image sensor makers, the message is clear: choose test platforms that are prepared for proprietary protocols, evolving standards, and ever-tighter time-to-market demands.&lt;/p&gt; 
&lt;p&gt;In this landscape, Teradyne’s modular hardware and powerful, agile software ensure that manufacturers are meeting current demands and are prepared for whatever comes next. With early interface testing capabilities and scalable solutions that can adapt on the fly, Teradyne customers stay ahead of integration risks, control costs, and accelerate time-to-market.&lt;/p&gt; 
&lt;p&gt;In an industry where speed, innovation, and reliability are everything, that kind of flexibility is more than just a technical feature. It’s a strategic necessity that offers manufacturers the freedom to innovate, knowing they have the flexibility they need in their test solutions.&lt;/p&gt; 
&lt;p&gt;The post &lt;a href=&quot;https://semiengineering.com/invisible-interfaces-the-hidden-challenge-behind-every-great-image-sensor/&quot;&gt;Invisible Interfaces: The Hidden Challenge Behind Every Great Image Sensor&lt;/a&gt; appeared first on &lt;a href=&quot;https://semiengineering.com&quot;&gt;Semiconductor Engineering&lt;/a&gt;.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://semiengineering.com/invisible-interfaces-the-hidden-challenge-behind-every-great-image-sensor/</guid>
      <pubDate>Tue, 09 Dec 2025 08:37:47 GMT</pubDate>
    </item>

    <item>
      <title>Resilient And Optimized GenAI Systems</title>
      <link>https://semiengineering.com/resilient-and-optimized-genai-systems/</link>
      <description>&lt;p&gt;AI and data center systems are being pushed to their limits, with soaring complexity, nonstop inference workloads, and rising energy demands. Addressing these pressures requires more than incremental improvements, it calls for collaboration across the ecosystem. That’s why proteanTecs has joined forces with Arm, bringing our real-time monitoring technology into Arm’s Neoverse Compute Subsystems (CSS). Successful integration brings a customer-ready solution – designed to accelerate power efficiency, performance, and reliability at scale.&lt;/p&gt; 
&lt;h2&gt;Challenges facing next-gen AI infrastructure&lt;/h2&gt; 
&lt;p&gt;The cloud AI landscape is at an inflection point. Explosive growth in model complexity, inference demand, and system scale has strained the very fabric of compute infrastructure. Training runs that once required thousands of GPUs now demand tens of thousands, with costs reaching hundreds of millions of dollars. Inference, once considered “easier,” now drives massive daily workloads that &lt;a href=&quot;https://hubs.la/Q03JqxL50&quot;&gt;push energy budgets and hardware reliability to the brink&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
&lt;li&gt;&lt;strong&gt;Power efficiency&lt;/strong&gt;: AI data centers will consume over 90 TWh annually by 2026. Excessive voltage guard bands, designed for worst-case scenarios, drive unnecessary energy waste.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ul&gt; 
&lt;li&gt;&lt;strong&gt;Performance at scale&lt;/strong&gt;: Even small throughput inefficiencies cascade at hyperscale. A 10% gain in throughput can reduce training times by weeks and save millions in infrastructure costs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ul&gt; 
&lt;li&gt;&lt;strong&gt;Reliability and resilience&lt;/strong&gt;: Silent Data Corruption (SDC) is an invisible risk. A single undetected error can corrupt weights across thousands of GPUs, invalidating billion-dollar training runs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For hyperscalers, the stakes are clear: every watt saved, every percentage of performance reclaimed, and every silent error prevented translates into millions of dollars and competitive advantage.&lt;/p&gt; 
&lt;p&gt;Meeting these challenges requires more than node upgrades or incremental optimizations. It demands in-situ visibility into how chips behave under real workloads and operating conditions, and the ability to act on that knowledge in real time.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://i0.wp.com/semiengineering.com/wp-content/uploads/2025/12/proteanTecs_Resilient-and-Optimized-GenAI-Systems-fig1.webp?resize=903%2C345&amp;amp;ssl=1&quot; alt=&quot;&quot; width=&quot;903&quot; height=&quot;345&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Growth in transistor density versus the PFLOPS required to train AI models from a 2021 baseline. By 2024, AI compute requirements surged by 6847%, while transistor density grew by only 183%. 2025 value is based on the projected PFLOPS required to train GPT-5. Source: Mollick, E. (2024). Scaling: The state of play in AI. One Useful Thing.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Deep data needed to face these challenges&lt;/h2&gt; 
&lt;p&gt;Current methods for optimizing performance, power, and reliability all share the same blind spot: they don’t see how chips behave under actual workloads in the field. GenAI cloud operators pay for this lack of real-time visibility through higher power draw, lower throughput, and increased risk of failure. Performance tuning relies on static margins. Power controls are triggered by basic telemetry. Reliability checks happen too late, after failure is already underway. None of these approaches adapts to actual stress and environmental conditions during live operation.&lt;/p&gt; 
&lt;p&gt;That’s the gap.&lt;/p&gt; 
&lt;p&gt;proteanTecs closes this gap by providing deep data monitoring solutions that give system designers and operators unprecedented visibility into chip health and performance throughout the lifecycle.&lt;/p&gt; 
&lt;p&gt;The technology delivers a complete monitoring solution spanning silicon to system. At the hardware level, an on-chip HW IP Monitoring System combines lightweight Agents with built-in infrastructure for seamless access, control, and integration, enabling deep visibility from within the silicon. Complementing this are advanced EDA-based integration and implementation tools that ensure high coverage and smooth deployment with no design impact. On top of the hardware, a suite of machine learning–driven software applications run in the field and in real time, providing predictive monitoring.&lt;/p&gt; 
&lt;p&gt;By embedding Agents within the silicon, we enable performance improvements, power reduction, and diagnostics throughout the device’s mission.&lt;/p&gt; 
&lt;p&gt;The on-chip Agents provide parametric measurements in-situ and in functional mode, to detect timing issues, operational and environmental effects, aging and application stress. Among the suite of Agents are the Margin Agents that monitor timing margins of millions of real paths for more informed decisions. Margin Agents provide very high coverage of the design’s logic and monitor the real performance-limiting paths that traditional methods often miss. The real performance-limiting (minimum voltage or maximum frequency) paths are ensured to be covered for all devices in the process distribution, and for all the operating conditions and functional workloads.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://i0.wp.com/semiengineering.com/wp-content/uploads/2025/12/proteanTecs_Resilient-and-Optimized-GenAI-Systems-fig2.webp?resize=373%2C203&amp;amp;ssl=1&quot; alt=&quot;&quot; width=&quot;373&quot; height=&quot;203&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unlike canary circuits (right, in yellow), proteanTecs uses on-chip Margin Agents (left, in blue) that monitor true critical paths.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Customer-ready integration&lt;/h2&gt; 
&lt;p&gt;Now, in collaboration with Arm, we’re bringing these capabilities directly into the heart of next-generation data center and AI infrastructure. &lt;a href=&quot;https://hubs.la/Q03Jqy040&quot;&gt;As part of Arm Total Design&lt;/a&gt;, proteanTecs has successfully integrated its monitoring solutions into Arm’s Neoverse Compute Subsystems (CSS). This milestone means our Agent integration is validated and optimized for Neoverse CSS, enabling mutual customers to benefit from seamless integration into their custom SoCs.&lt;/p&gt; 
&lt;p&gt;This milestone means:&lt;/p&gt; 
&lt;ul&gt; 
&lt;li&gt;Customer-ready integration: proteanTecs monitoring solutions are now natively available within Neoverse CSS-based custom SoCs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ul&gt; 
&lt;li&gt;Preferential access: As a member of Arm Total Design, proteanTecs gains early access to Neoverse CSS, enabling deep integration and joint validation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ul&gt; 
&lt;li&gt;Faster time-to-market: Mutual customers benefit from seamless adoption – cutting integration effort, validation cycles, and deployment risk.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The result: system designers can bring powerful AI/data center SoCs to market faster, with embedded visibility, power/performance optimization, and reliability monitoring built-in.&lt;/p&gt; 
&lt;h2&gt;Demonstrating coverage, efficiency, and seamless integration&lt;/h2&gt; 
&lt;p&gt;The integration of proteanTecs monitoring solutions into Arm’s Neoverse CSS has now been validated in practice, and the results underscore the value of a customer-ready reference design.&lt;/p&gt; 
&lt;p&gt;In this implementation, in an advanced process node, 200 Margin Agents (MAs) were integrated and implemented in one of the most advanced Arm Neoverse CPU cores. proteanTecs proprietary algorithms, part of proteanTecs EDA tools, provide the decision on which endpoints should be monitored by each Margin Agent. This ensures that the true performance-limiting paths are monitored.&lt;/p&gt; 
&lt;p&gt;This strategic monitoring achieved a coverage result of 96.63% (based on proteanTecs proprietary coverage metrics), a level of visibility that allows customers to make confident, data-driven decisions. For more information about proteanTecs’ coverage methodology, customers are encouraged &lt;a href=&quot;https://www.proteantecs.com/contact-us&quot;&gt;to reach out to our support team&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://i0.wp.com/semiengineering.com/wp-content/uploads/2025/12/proteanTecs_Resilient-and-Optimized-GenAI-Systems-fig3.webp?resize=788%2C244&amp;amp;ssl=1&quot; alt=&quot;&quot; width=&quot;788&quot; height=&quot;244&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Equally important, the addition of monitoring capability had virtually no effect on the design itself. Timing and power measurements remained stable and well within normal run-to-run variation, confirming that the integration does not compromise efficiency. Max timing and power results are shown in the table below.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://i0.wp.com/semiengineering.com/wp-content/uploads/2025/12/proteanTecs_Resilient-and-Optimized-GenAI-Systems-fig4.webp?resize=841%2C309&amp;amp;ssl=1&quot; alt=&quot;&quot; width=&quot;841&quot; height=&quot;309&quot;&gt;&lt;/p&gt; 
&lt;p&gt;No manual timing fixes were applied, so the results reflect a true Synthesis and Place-and-Route tools output, ensuring transparency and reliability in the process.&lt;/p&gt; 
&lt;p&gt;Taken together, these findings provide customers with a reference implementation that demonstrates how proteanTecs can be embedded seamlessly into high-speed designs at advanced process nodes, without introducing overhead or risk.&lt;/p&gt; 
&lt;p&gt;proteanTecs’ solution is an open architecture and can work under partner monitoring frameworks. Among the supported frameworks is the &lt;a href=&quot;https://developer.arm.com/community/arm-community-blogs/b/servers-and-cloud-computing-blog/posts/system-monitoring-control-framework-arm-neoverse-css&quot;&gt;Arm System Monitoring Control Framework (SMCF)&lt;/a&gt;, which enhances monitoring for Arm CSS solutions. You can learn more about proteanTecs’ integration with SMCF &lt;a href=&quot;https://www.proteantecs.com/blog/expanding-the-horizon-of-system-monitoring-with-the-arm-smcf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Unlocking efficiency, performance, and reliability&lt;/h2&gt; 
&lt;p&gt;proteanTecs’ suite of applications, now enabled for Neoverse CSS, ensure datacenter operators can optimize at runtime:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AVS Pro&lt;/strong&gt;: Workload and reliability aware, real-time power reduction – delivering up to 14% lower power with no performance loss, while extending the device RUL by ~20%. To learn more, read the &lt;a href=&quot;https://hubs.la/Q03JqyC40&quot;&gt;white paper&lt;/a&gt; here.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AFS Pro&lt;/strong&gt;: Workload and reliability aware, real-time frequency increase – capturing frequency headroom for up to 10% performance boost.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;RTHM&lt;/strong&gt;: Monitors health in real-time, flagging risks before they cascade into SDC or system failures. &lt;a href=&quot;https://hubs.la/Q03JqyWx0&quot;&gt;Read more here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://i0.wp.com/semiengineering.com/wp-content/uploads/2025/12/proteanTecs_Resilient-and-Optimized-GenAI-Systems-fig5.webp?resize=582%2C414&amp;amp;ssl=1&quot; alt=&quot;&quot; width=&quot;582&quot; height=&quot;414&quot;&gt;&lt;/p&gt; 
&lt;p&gt;By embedding these capabilities into Neoverse CSS-based SoCs, mutual customers gain a powerful edge: the ability to scale AI infrastructure power efficiency, performance, and reliability.&lt;/p&gt; 
&lt;h2&gt;Conclusion: Real-time monitoring for scalable GenAI chips&lt;/h2&gt; 
&lt;p&gt;As GenAI chips reach unprecedented levels of complexity, chipmakers need visibility into how each chip truly behaves under live workloads.&lt;/p&gt; 
&lt;p&gt;proteanTecs delivers exactly that, with a new class of in-chip monitoring and applications that dynamically tune in real-time each device for optimal efficiency, performance, and RAS. Now, through successful integration with Arm’s Neoverse Compute Subsystems (CSS) as part of Arm Total Design, proteanTecs’ real-time monitoring solutions are validated, optimized, and customer-ready. This seamless integration enables mutual customers to accelerate time-to-market while benefiting from power reduction, performance improvement, and built-in resilience at hyperscale.&lt;/p&gt; 
&lt;p&gt;The post &lt;a href=&quot;https://semiengineering.com/resilient-and-optimized-genai-systems/&quot;&gt;Resilient And Optimized GenAI Systems&lt;/a&gt; appeared first on &lt;a href=&quot;https://semiengineering.com&quot;&gt;Semiconductor Engineering&lt;/a&gt;.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://semiengineering.com/resilient-and-optimized-genai-systems/</guid>
      <pubDate>Tue, 09 Dec 2025 08:37:47 GMT</pubDate>
    </item>

    <item>
      <title>Boosting Production Performance: Ensuring Only Known-Good Sockets Enter Your Line</title>
      <link>https://semiengineering.com/boosting-production-performance-ensuring-only-known-good-sockets-enter-your-line/</link>
      <description>&lt;p&gt;Efficient, stable, and high-yield semiconductor production depends on one often-overlooked factor: the health of your test sockets.&lt;/p&gt; 
&lt;p&gt;In many factories, socket maintenance and inspection practices haven’t kept pace with the demands of today’s high-density, high-speed packages. The result? Hidden marginal pins, unexpected downtime, multisite yield variation, and inflated manufacturing costs.&lt;/p&gt; 
&lt;p&gt;In this post, we explore how Modus Test’s MPT and MTC transform socket management by ensuring only &lt;strong&gt;known-good sockets (KGS)&lt;/strong&gt; are used on the production floor.&lt;/p&gt; 
&lt;h2&gt;The hidden problem: Why traditional socket maintenance falls short&lt;/h2&gt; 
&lt;p&gt;Factories typically receive two kinds of sockets:&lt;/p&gt; 
&lt;ul&gt; 
&lt;li&gt;New sockets shipped directly from suppliers&lt;/li&gt; 
&lt;li&gt;Used sockets returning from the production floor&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Both are assumed to be in good condition—either because they are brand new or because they didn’t show major yield issues in previous test runs.&lt;/p&gt; 
&lt;p&gt;But the reality is more complicated.&lt;/p&gt; 
&lt;h3&gt;1. Supplier outgoing tests often miss marginal CRES pins&lt;/h3&gt; 
&lt;p&gt;Daisy-chain continuity tests used by many socket suppliers suffer from &lt;a href=&quot;https://semiengineering.com/enhancing-test-socket-performance-through-application-specific-validation-and-system-level-per-pin-oqc/&quot;&gt;an averaging effect&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This means some out-of-spec pins can escape detection, passing OQC even though their contact resistance (CRES) is failing.&lt;/p&gt; 
&lt;h3&gt;2. Hardware shops can’t detect electrical failures&lt;/h3&gt; 
&lt;p&gt;Incoming inspection at the hardware shop is often visual or mechanical only. As a result:&lt;/p&gt; 
&lt;ul&gt; 
&lt;li&gt;Solder-fatigued pins&lt;/li&gt; 
&lt;li&gt;Worn pogo pins&lt;/li&gt; 
&lt;li&gt;Oxidized or contaminated contacts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;…may go unnoticed.&lt;/p&gt; 
&lt;h3&gt;3. Returning used sockets often contain hidden failing pins&lt;/h3&gt; 
&lt;p&gt;When these problematic sockets are released back into production, the consequences add up fast:&lt;/p&gt; 
&lt;ul&gt; 
&lt;li&gt;Extra setup time&lt;/li&gt; 
&lt;li&gt;Unexpected equipment downtime&lt;/li&gt; 
&lt;li&gt;Low first-pass yield&lt;/li&gt; 
&lt;li&gt;High retest rates&lt;/li&gt; 
&lt;li&gt;Multisite yield mismatch&lt;/li&gt; 
&lt;li&gt;Throughput drops and capacity shortages&lt;/li&gt; 
&lt;li&gt;Rising manufacturing costs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This is exactly the problem Modus Test KGS was designed to solve.&lt;/p&gt; 
&lt;h2&gt;Introducing a known-good socket workflow&lt;/h2&gt; 
&lt;p&gt;The ModusTest Known-Good Socket (KGS) process leverages both the MPT and MTC to guarantee that only electrically verified, known-good sockets enter the production line.&lt;/p&gt; 
&lt;h3&gt;&lt;img src=&quot;https://s.w.org/images/core/emoji/16.0.1/72x72/2714.png&quot; alt=&quot;✔&quot;&gt; Outgoing Quality Check (OQC)&lt;/h3&gt; 
&lt;p&gt;Ensure that every socket—new or refurbished—meets electrical standards before being released to the floor.&lt;/p&gt; 
&lt;h3&gt;&lt;img src=&quot;https://s.w.org/images/core/emoji/16.0.1/72x72/2714.png&quot; alt=&quot;✔&quot;&gt; Incoming Quality Check (IQC)&lt;/h3&gt; 
&lt;p&gt;Verify that:&lt;/p&gt; 
&lt;ul&gt; 
&lt;li&gt;Only known-good sockets are returned to the hardware shop&lt;/li&gt; 
&lt;li&gt;New sockets from suppliers actually meet expectations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Result:&lt;/h3&gt; 
&lt;p&gt;Your production line only ever sees electrically verified good sockets—not just visually “good” ones.&lt;/p&gt; 
&lt;h2&gt;Major Benefits of MPT &amp;amp; MTC in Socket Management&lt;/h2&gt; 
&lt;h3&gt;1. Dramatic improvement in OEU, OEE, FPY, and test yield&lt;/h3&gt; 
&lt;p&gt;Known-good sockets reduce false failures, prevent retests, and stabilize multisite matching across handlers.&lt;/p&gt; 
&lt;h3&gt;2. Elimination of package-specific hand socket lids&lt;/h3&gt; 
&lt;p&gt;The MTC’s high-force capability removes the need for custom lids and lid maintenance—saving significant cost and complexity.&lt;/p&gt; 
&lt;h3&gt;3. Customized binning through MPT&lt;/h3&gt; 
&lt;p&gt;MPT’s bin-setting capability allows operators to:&lt;/p&gt; 
&lt;ul&gt; 
&lt;li&gt;Apply different CRES limits per pin application&lt;/li&gt; 
&lt;li&gt;Extend socket life safely&lt;/li&gt; 
&lt;li&gt;Reduce maintenance costs&lt;/li&gt; 
&lt;li&gt;Improve pin replacement decisions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;4. Instant visualization via 2D &amp;amp; 3D pin maps&lt;/h3&gt; 
&lt;p&gt;Linked to test results, these maps let engineers spot failing or marginal pins immediately—no guesswork, no delays.&lt;/p&gt; 
&lt;h3&gt;5. High-throughput testing for connectors&lt;/h3&gt; 
&lt;p&gt;With:&lt;/p&gt; 
&lt;ul&gt; 
&lt;li&gt;High pin count (MPT)&lt;/li&gt; 
&lt;li&gt;High mechanical force (MTC)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;…factories can implement large-scale OQC for high-density connectors as well.&lt;/p&gt; 
&lt;h2&gt;Real-world customer results&lt;/h2&gt; 
&lt;p&gt;Customers using the ModusTest KGS workflow report dramatic performance improvements:&lt;/p&gt; 
&lt;ul&gt; 
&lt;li&gt;One customer increased setup stability from 1 day → 5–7 days&lt;/li&gt; 
&lt;li&gt;Another improved from 3 days → 7 days&lt;/li&gt; 
&lt;li&gt;Reduced equipment downtime&lt;/li&gt; 
&lt;li&gt;Optimized performance binning (speed &amp;amp; voltage) results&lt;/li&gt; 
&lt;li&gt;Stabilized multisite yield&lt;/li&gt; 
&lt;li&gt;Increased throughput and capacity&lt;/li&gt; 
&lt;li&gt;Significant annual manufacturing cost savings&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These improvements are not theoretical—they are real, measurable outcomes achieved in high-volume production lines.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;The health of your test sockets directly determines the health of your production line. With Modus Test MPT and MTC, factories can finally close the quality gap in socket inspection—ensuring every socket entering the line is truly known-good.&lt;/p&gt; 
&lt;p&gt;The result is simple:&lt;/p&gt; 
&lt;ul&gt; 
&lt;li&gt;Higher yield&lt;/li&gt; 
&lt;li&gt;Lower cost&lt;/li&gt; 
&lt;li&gt;More throughput&lt;/li&gt; 
&lt;li&gt;Happier engineers&lt;/li&gt; 
&lt;li&gt;Stabilized test performance across the board&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;iframe allowfullscreen=&quot;allowfullscreen&quot; src=&quot;https://www.youtube.com/embed/e-XsCFEkKBs&quot; width=&quot;480&quot; height=&quot;270&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;The post &lt;a href=&quot;https://semiengineering.com/boosting-production-performance-ensuring-only-known-good-sockets-enter-your-line/&quot;&gt;Boosting Production Performance: Ensuring Only Known-Good Sockets Enter Your Line&lt;/a&gt; appeared first on &lt;a href=&quot;https://semiengineering.com&quot;&gt;Semiconductor Engineering&lt;/a&gt;.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://semiengineering.com/boosting-production-performance-ensuring-only-known-good-sockets-enter-your-line/</guid>
      <pubDate>Tue, 09 Dec 2025 08:37:47 GMT</pubDate>
    </item>

    <item>
      <title>Metrology Digs Deep To Produce Next-Generation 3D NAND</title>
      <link>https://semiengineering.com/metrology-digs-deep-to-produce-next-generation-3d-nand/</link>
      <description>&lt;p&gt;Each generation of 3D NAND packs about 30% more bits than the previous version, with current devices storing up to 2 terabits of data in a die the size of a fingernail. With new product introductions shrinking from 18 months to every 12 months, chipmakers are constantly innovating to enable this prodigious scaling pace.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://semiengineering.com/knowledge_centers/memory/non-volatile-memory-nvm/flash-memory/3d-nand/&quot;&gt;3D NAND&lt;/a&gt; technology is a core ingredient in mobile phones, solid-state drives, data centers, PCs and SD cards. Over 30% of capital equipment orders each year go toward toolsets for building flash chips. In addition to its important role in global spending, 3D NAND drives 3D metrology and inspection technology to new extremes. This requires a combination of technologies, including optical, X-ray, high-landing-energy e-beam and e-beam voltage contrast, while brand new methods such as GaN-based e-beam are being introduced for their ability to detect critical defects.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Scaling bit density&lt;/strong&gt;&lt;br&gt; 
Ever since 3D NAND started playing a role in enterprise-grade solid-state drives, chipmakers have been using gate-all-around charge-trap cells. Those are somewhat similar to floating-gate cells, but with different voltage patterns moving electrons in and out of the trapping layer. Silicon nitride forms the trap cells.&lt;/p&gt; 
&lt;p&gt;Silicon nitride is a good choice for isolation because it is less susceptible to defects and leakage than polysilicon, and it requires lower voltage to support program/erase cycles. Because of this, the charge-trap cell can use a thinner oxide layer and reduce stress on the layer, which results in higher endurance rates than with floating gate cells. The charge-trap approach also enables faster read and write operations and consumes less energy. [2]&lt;/p&gt; 
&lt;p&gt;To fabricate 3D NAND devices, chipmakers deposit multiple horizontal film layers of memory cells, into which vertical channel hole connections are made. To increase memory capacity, more but thinner films of oxide-nitride are stacked in 2 or 3 tiers. Cryogenic etching systems, offered by Lam Research and TEL, play a critical role in producing high-aspect-ratio holes &amp;lt;100nm in diameter and 6 to 10 microns deep, because at ultra-low temperatures (down to -60°C), etch rates are substantially faster because reactive species have higher concentration and removal capability at the etch front. An amorphous carbon hard mask aids the vertical etch. Challenges include preventing bowing, twisting, or tilting while achieving near-vertical profiles.&lt;/p&gt; 
&lt;p&gt;The scaling of 3D NAND is progressing on three fronts. First, NAND manufacturers are scaling the pitch between contact holes, enabling a greater number of memory cells in the same silicon footprint. Second, stacks are growing vertically by adding more layers and tiers of oxide/word lines. Third is the so-called logical scaling. Here, more bits are packed into each cell, moving from triple-level cells to quad-level cells and penta-level cells (TLC, QLC and PLC, respectively), each operating at different threshold voltages.&lt;/p&gt; 
&lt;p&gt;The combination of horizontal and vertical scaling requires absolute profile control from the etching process. “If the hole size and hole shape are not perfect, you will have interference from nearby devices so that logical scaling will not be possible,” said Tae Won Kim, corporate vice president of global products at &lt;a href=&quot;https://semiengineering.com/entities/lam-research/&quot;&gt;Lam Research&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Among all the important features in 3D NAND (see figure 1), including memory holes, slits, staircase contacts, and peripheral contacts, the vertical memory holes are the smallest. “Customers want high-resolution z-profiles of channel holes, wordline cut trenches and hard mask holes,” said Nick Keller, director of applications development, optical metrology at &lt;a href=&quot;https://semiengineering.com/entities/onto-innovation/&quot;&gt;Onto Innovation&lt;/a&gt;. “There are also steps where they want to know the vertical recess in etch back steps that occur at the bottom of the channel holes (or at the very top).”&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://i0.wp.com/semiengineering.com/wp-content/uploads/2025/12/Screenshot-2025-12-08-at-2.19.57-PM.png?resize=549%2C346&amp;amp;ssl=1&quot; alt=&quot;&quot; width=&quot;549&quot; height=&quot;346&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fig. 1: Key features in 3D NAND include tiny memory holes, slits, staircase contacts, and peripheral contacts. Source: Lam Research&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Stacking cells vertically has several important benefits. It provides a higher bit density and improves electrical performance by shortening the interconnect length between cells, which in turn reduces power consumption. The nitride layers are sacrificial, so they are removed using a wet etch and then replaced with metal.&lt;/p&gt; 
&lt;p&gt;With the gate-last integration, the word line is formed using tungsten. The tungsten replacement and separation processes can create several defect types, including tungsten voids, oxide voids, and bridging defects.&lt;/p&gt; 
&lt;p&gt;The integration of the word line is a particular challenge for process integrators and for defect inspection and reduction. Techniques like CD-SEM or atomic force microscopy (AFM) have a difficult time seeing inside features. The main defects of interest are typically sub-surface (especially voids) in the stack, or defects caused by residue after etching at the bottom of high-aspect-ratio memory holes.&lt;/p&gt; 
&lt;p&gt;Another key metrology step follows the tungsten etch or the tungsten recess step. This juncture is critical because under-etch of the tungsten can lead to word-line shorting and destruction of the memory string. Over-etching typically causes degradation in device performance.&lt;/p&gt; 
&lt;p&gt;Once the memory cell stacks are fabricated, they are connected to the control logic below using wafer bonding in the latest 3D NAND chips. Acoustic microscopy is a common method used to detect voids at the critical wafer-wafer interface.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The best tool for the job&lt;/strong&gt;&lt;br&gt; 
Scatterometry, also known as optical CD (OCD), is widely used throughout fabs. Years ago, these techniques were extended from the visible into the infrared wavelengths to enable better measurement inside vertical holes.&lt;/p&gt; 
&lt;p&gt;“For 3D NAND, IRCD has been proven in HVM for the high-aspect-ratio z-profile measurements,” said Keller. He noted that while critical dimension small-angle x-ray scattering (CD-SAXS) also has been proven in manufacturing settings, especially for tilt and overlay measurements between strings (tiers), the throughput of CD-SAXS cannot compete with that of optical methods. So it may be that CD-SAXS is employed selectively in areas where other methods cannot provide the same information, such as ensuring good overlay from one memory hole tier to another.&lt;/p&gt; 
&lt;p&gt;Infrared critical dimension metrology (IRCD) offers vertical profile sensitivity simply due to the wavelength range it operates in and the dielectric layers that are in the superlattice (silicon nitride and silicon dioxide). Like OCD, IRCD is an indirect measurement that relies on a Muller Matrix correlation of the spectral response to CD measurements. To meet the needs of high-volume manufacturing, IRCD needs to capture within-wafer CD variations, as well as wafer-to-wafer, at a processing speed comparable to that of OCD systems.&lt;/p&gt; 
&lt;p&gt;“Dielectric materials in the mid-wavelength and long-wavelength infrared ranges have absorption peaks that depend on the types of bonds found in the molecule (for SiO2, it would be the strong Si-O bond due to “stretching” around 1000cm-1). The absorption peaks have amplitude and width that vary with wavelength and therefore modulate the penetration depth of light,” explained Keller. “Second, compared to UV-VIS-NIR spectroscopic instruments, creating an OCD model in the IR is faster due to fewer high-frequency oscillations, which is computationally advantageous because less harmonics are required in the RCWA (rigorous coupled wave analysis) calculation.”&lt;/p&gt; 
&lt;p&gt;IRCD can be used to measure channel hole CD and the silicon nitride recess on first- and second-tier channel holes. Measuring the nitride recess is critical because confinement in self-aligned charge trap layers helps improve data retention in the memory device by preventing lateral charge migration.&lt;/p&gt; 
&lt;p&gt;A fundamental challenge with extending existing tools to measure critical dimensions in deep features is the signal absorption by the thin films on the side. “If the film layers have no absorption, there is no theoretical limit, but modeling can become challenging due to loss of spectral resolution or parameter correlation,” said Keller.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;E-beams and X-rays&lt;/strong&gt;&lt;br&gt; 
Electron-beam tools, in general, provide a more detailed review of defects identified using optical systems. In recent years, Applied Materials and KLA have developed high landing energy systems to zoom into high-aspect-ratio holes.&lt;/p&gt; 
&lt;p&gt;E-beam metrology with high landing energy in the 30keV region can penetrate high AR holes. Detection of both backscattered and secondary electrons can identify defects of interest several microns deep. High landing energy e-beam can indicate conditions of remaining tungsten in memory holes. Deep learning helps the defect classification process during ADC runs, while also discriminating between nuisance defects and killer defects.&lt;/p&gt; 
&lt;p&gt;The high-landing-energy e-beam system from Applied Materials uses a cold field emission gun to deliver more electrons to the structure in a narrower beam. This tool can provide up to 60keV landing energy.&lt;/p&gt; 
&lt;p&gt;But chipmakers are exercising caution in the use of high landing energy e-beam, because ionizing radiation can induce damage in sensitive NAND layers — especially the dielectric stacks. Any inadvertent change to the charge-trap region can affect the device threshold voltages, degrading performance or long-term reliability.&lt;/p&gt; 
&lt;p&gt;Voltage-contrast inspection with an e-beam point-scan system provides an effective means of identifying critical hot spots. This method is often applied during the device learning or ramping stages when hot spots are extremely rare but present.&lt;/p&gt; 
&lt;p&gt;“When people say, ‘I want to look at a hot spot,’ they’re thinking it’s something you can look at — one or two of them — and see the problem. But stochastic defects have both a deterministic element and a statistical element,” said John Kibarian, president and CEO of &lt;a href=&quot;https://semiengineering.com/entities/pdf-solutions/&quot;&gt;PDF Solutions&lt;/a&gt;. “So the reality is, I‘ve got to look at 10 or 20 billion of them to see just one that fails. That hot spot has a failure rate that is above the floor level, but it’s still very, very infrequent. And that’s the mining you’re looking for with hot spots these days.”&lt;/p&gt; 
&lt;p&gt;Software plays a key role in ferreting out these hot spots. “That’s really the challenge — identifying where to look in the e-beam tool to give you that ability to look at tens of billions of spots in a reasonable amount of time,” Kibarian said.&lt;/p&gt; 
&lt;p&gt;Yet another e-beam technique with an alternative source may prove useful in identifying NAND defects. Kioxa recently announced that it is evaluating a GaN-based e-beam tool for defect inspection in 3D NAND applications. [3] Jointly developed by Nagoya University’s Amano-Honda laboratory and Photo Electron Soul, a start-up firm, the GaN e-beam system promises non-contact defect inspection, electrical inspection, and profile measurements. The system uses selective e-beam radiation and real-time control of beam intensity to minimize any beam misalignment to enhance defect detection and root-cause analysis of failures.&lt;/p&gt; 
&lt;p&gt;Another promising option for seeing defects inside high AR holes are X-ray methods including X-ray CT (computed tomography). X-ray inspection in general benefits from recent improvements in X-ray source and detection methods. “With the change to gate-all-around [structures], there is this increased need for metrology,” said Juliette van der Meer, product marketing manager at &lt;a href=&quot;https://semiengineering.com/entities/bruker/&quot;&gt;Bruker&lt;/a&gt;. “We met it with the launch of a new X-ray tool with increased source power and a better detector to keep up with the needs in high-volume manufacturing.”&lt;/p&gt; 
&lt;p&gt;Finally, after the memory cell structure is built, NAND chipmakers will bond the “CMOS under array” chip to the NAND memory cells. Here, acoustic microscopy is proving useful in identifying small voids between wafers bonded in a hybrid or fusion bonding process.&lt;/p&gt; 
&lt;p&gt;Acoustic microscopy, which sends a high-frequency signal through a water medium, identifies defects in bonded wafers. One approach taken by Nordson Test &amp;amp; Measurement involves spinning the wafer at high speed while using a waterfall transducer to provide non-immersion scanning, minimizing the risk of contamination or false-bond indications. The transducer type (frequency) is chosen based on the application, providing a range of focal lengths for detecting voids in bonded wafer or bonded chip-on-wafer applications.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Ground truth verification&lt;/strong&gt;&lt;br&gt; 
While all the above methods are non-destructive in nature, destructive approaches enable device cross-sectioning using focused ion beam milling coupled with a scanning electron microscope (FIB-SEM) to view actual cross-sections of device wafers. During process development and ramping, FIB-SEM methods can reveal incomplete etching of features, bowing, or twisting, and especially channel hole-to-channel hole variations.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Process modeling/virtual metrology&lt;/strong&gt;&lt;br&gt; 
The growing challenge behind characterizing many of the subtle and complicated yield-limiting factors of severely scaled features is making virtual wafer fabrication, process modeling, and virtual metrology increasingly attractive. The combination can help identify the best metrology sampling rate as a device enters production. Chipmakers can evaluate tradeoffs between metrology precision and speed of measurements. Virtual fabrication also has the potential to accelerate the semiconductor development cycle by substituting limited and lengthy wafer-based design-of-experiments with fast, large-scale virtual DOEs.&lt;/p&gt; 
&lt;p&gt;Engineers from Coventor, a Lam Research company, quantified the extent of hole CD variation and channel taper from the top of the channel to the bottom using virtual process modeling and metrology. [4] They modeled a stack of alternating layers of oxide/nitride and calculated the channel area at the top and bottom of the channel as a function of the high-aspect-ratio etch process.&lt;/p&gt; 
&lt;p&gt;“For a nominal etch condition, the CD variation from the top to the bottom of the channel can readily be seen and numerically quantified. To ensure that the channel etch reaches the bottom contact, the sidewall angle for each stack layer must be &amp;gt;88°, or the etch does not reach the bottom of the channel. By incorporating this type of virtual metrology and statistical process variation, the process boundaries of various channel etch process parameters can be optimized prior to running excessive trial-and-error silicon wafers,” stated the authors.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;br&gt; 
3D NAND devices push the envelope in metrology measurements and inspection of defects. With its abundance of high-aspect-ratio holes and challenging etch processes, chipmakers employ a combination of IRCD, X-ray inspection, e-beam methods with high landing energies, and e-beam voltage contrast to “see” inside deep features, looking for residues, incomplete etching processes, as well as bowing of features created by defects and non-vertical profiles.&lt;/p&gt; 
&lt;p&gt;As companies like Kioxa, Samsung, Micron and SK hynix prepare to roll out next-generation NAND with higher stacks, smaller slits, and smaller memory holes, it will take an all-hands-on-deck approach to reach yield entitlement across these complex memory arrays.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
&lt;li&gt;Moyer, “NAND Flash Targets 1,000 Layers,” Semiconductor Engineering, December 4, 2024, &lt;a href=&quot;https://semiengineering.com/nand-flash-targets-1000-layers/&quot;&gt;https://semiengineering.com/nand-flash-targets-1000-layers/&lt;/a&gt;&lt;/li&gt; 
&lt;li&gt;Sheldon, “Charge-trap Technology Advantages for 3D NAND Flash Drives, Techtarget, June 19, 2023, &lt;a href=&quot;https://www.techtarget.com/searchstorage/tip/Charge-trap-technology-advantages-for-3D-NAND-flash-drives#:~:text=Because%20of%20the%20limitations%20that,operations%20and%20consumes%20less%20energy&quot;&gt;https://www.techtarget.com/searchstorage/tip/Charge-trap-technology-advantages-for-3D-NAND-flash-drives#:~:text=Because%20of%20the%20limitations%20that,operations%20and%20consumes%20less%20energy&lt;/a&gt;.&lt;/li&gt; 
&lt;li&gt;GaN-based e-Beam Inspection and Metrology Co-developed by Startup Photo electron Soul Inc. and Nagoya University Will be Evaluated by Kioxia, a Leading Flash Memory Producer,” September 1, 2025, press release. &lt;a href=&quot;https://www.businesswire.com/news/home/20250901390796/en/GaN-based-e-Beam-Inspection-and-Metrology-Co-developed-by-Startup-Photo-electron-Soul-Inc.-and-Nagoya-University-Will-be-Evaluated-by-Kioxia-a-Leading-Flash-Memory-Producer&quot;&gt;https://www.businesswire.com/news/home/20250901390796/en/GaN-based-e-Beam-Inspection-and-Metrology-Co-developed-by-Startup-Photo-electron-Soul-Inc.-and-Nagoya-University-Will-be-Evaluated-by-Kioxia-a-Leading-Flash-Memory-Producer&lt;/a&gt;&lt;/li&gt; 
&lt;li&gt;Hargrove, et. al., “Review of virtual wafer process modeling and metrology for advanced technology development,” &lt;a href=&quot;https://www.spiedigitallibrary.org/journals/journal-of-micro-nanopatterning-materials-and-metrology/volume-22/issue-3&quot;&gt;Journal of Micro/Nanopatterning, Materials, and Metrology, Vol. 22, Issue 3&lt;/a&gt;, 031209 (July 2023). &lt;a href=&quot;https://doi.org/10.1117/1.JMM.22.3.031209&quot;&gt;https://doi.org/10.1117/1.JMM.22.3.031209&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Related Reading&lt;/strong&gt;&lt;br&gt; 
&lt;a href=&quot;https://semiengineering.com/nand-flash-targets-1000-layers/&quot;&gt;NAND Flash Targets 1,000 Layers&lt;br&gt; 
&lt;/a&gt;New techniques go beyond improved deposition and etching, but challenges stack up, too.&lt;br&gt; 
&lt;a href=&quot;https://semiengineering.com/how-metrology-tools-stack-up-in-3d-nand-devices/&quot;&gt;How Metrology Tools Stack Up In 3D NAND Devices&lt;br&gt; 
&lt;/a&gt;Buried features and re-entrant geometries drive application-specific metrology solutions.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;The post &lt;a href=&quot;https://semiengineering.com/metrology-digs-deep-to-produce-next-generation-3d-nand/&quot;&gt;Metrology Digs Deep To Produce Next-Generation 3D NAND&lt;/a&gt; appeared first on &lt;a href=&quot;https://semiengineering.com&quot;&gt;Semiconductor Engineering&lt;/a&gt;.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://semiengineering.com/metrology-digs-deep-to-produce-next-generation-3d-nand/</guid>
      <pubDate>Tue, 09 Dec 2025 08:37:47 GMT</pubDate>
    </item>

    <item>
      <title>Overview Of Radiation Dose During X-ray Inspection Of Electronics</title>
      <link>https://semiengineering.com/overview-of-radiation-dose-during-x-ray-inspection-of-electronics/</link>
      <description>&lt;p&gt;X-ray imaging of semiconductor and electronic devices is an invaluable tool; enabling non-invasive sub-surface inspection, identification of defects and measurement of critical dimensions. Figure 1 shows a schematic and description of a typical X-ray inspection system for electronics and semiconductor devices. Unfortunately, semiconductor devices are sensitive to sustained radiation dose, which if too high, can cause current leakage, damage, and ultimately failure of the component or device. This overview paper describes the fundamental mechanism for radiation damage in semiconductor devices; typical dose tolerance limits of electronics; how to measure and model radiation dose levels; and methods for reducing dose during X-ray inspection.&lt;/p&gt; 
&lt;p&gt;Read &lt;a href=&quot;https://www.nordson.com/en/divisions/test-and-inspection/ti_knowledge-center/data/form-page_overview-of-radiation-dose-during-x-ray-inspection-of-electronics&quot;&gt;more here.&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://i0.wp.com/semiengineering.com/wp-content/uploads/2025/12/Screenshot-2025-12-08-at-1.37.02-PM.png?resize=300%2C298&amp;amp;ssl=1&quot; alt=&quot;&quot; width=&quot;300&quot; height=&quot;298&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fig.1: Schematic of a typical electronics X-ray inspection system.  Source: Nordson&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The post &lt;a href=&quot;https://semiengineering.com/overview-of-radiation-dose-during-x-ray-inspection-of-electronics/&quot;&gt;Overview Of Radiation Dose During X-ray Inspection Of Electronics&lt;/a&gt; appeared first on &lt;a href=&quot;https://semiengineering.com&quot;&gt;Semiconductor Engineering&lt;/a&gt;.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://semiengineering.com/overview-of-radiation-dose-during-x-ray-inspection-of-electronics/</guid>
      <pubDate>Tue, 09 Dec 2025 08:37:47 GMT</pubDate>
    </item>

    <item>
      <title>Beyond The Core: Tackling System-Wide Debugging For Complex SoCs</title>
      <link>https://semiengineering.com/beyond-the-core-tackling-system-wide-debugging-for-complex-socs/</link>
      <description>&lt;p&gt;The world of System-on-Chips (SoCs) is evolving – with the advancement of generative AI, the increasing demand for high-performance compute, and the innovative shift towards multi-chiplet architectures, system complexity is advancing at an increased pace. And with complexity comes an even greater challenge: &lt;strong&gt;debugging complexity&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Silent data corruption, elusive timing-sensitive bugs, and intricate interactions across heterogeneous components are becoming increasingly difficult to root cause. Without the right kind of deep, system-wide visibility, engineers can find themselves in a frustrating cycle of trying to diagnose problems that seem to vanish and reappear, consuming valuable time.&lt;/p&gt; 
&lt;p&gt;In this blog, we introduce Tessent UltraSight and how it can help with debugging complexity.&lt;/p&gt; 
&lt;h2&gt;It’s not just about the processor core anymore, it’s also about the system&lt;/h2&gt; 
&lt;p&gt;Functional monitoring offers the essential capability to non-intrusively observe and collect data, ensuring that the entire system behaves as expected, thus accelerating debug efforts. This involves crucial activities like gathering bus transaction activity and statistics across your System-on-Chip (SoC) to pinpoint bottlenecks or unexpected behavior, or analyzing the behavior of critical hardware signals well beyond the immediate vicinity of a processor.&lt;/p&gt; 
&lt;p&gt;These insights provide the crucial context needed to root-cause issues that extend beyond the instructions executed by the processor core, offering a holistic view of your system’s operation.&lt;/p&gt; 
&lt;h2&gt;Debugging complex SoCs&lt;/h2&gt; 
&lt;p&gt;Modern SoCs are incredibly complex, getting real-time and deep visibility into their functional behavior can be a huge challenge. That’s where Tessent UltraSight comes in – Tessent UltraSight offers a complete functional monitoring and debugging solution, designed to streamline and accelerate engineering efforts throughout the entire SoC development lifecycle.&lt;/p&gt; 
&lt;ol&gt; 
&lt;li&gt;&lt;strong&gt;Embedded functional monitoring hardware IPs&lt;/strong&gt; 
&lt;ul&gt; 
&lt;li&gt;A suite of highly configurable, pre-verified, and silicon-proven hardware IPs that serve as your system’s built-in intelligence. These are the “eyes and ears” embedded directly into your design.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;/li&gt; 
&lt;li&gt;&lt;strong&gt;EA Builder for design engineers: Simplify generating UltraSight hardware subsystems with user-friendly interface &lt;/strong&gt; 
&lt;ul&gt; 
&lt;li&gt;This new tool helps reduce integration time for design engineers by generating functional monitoring and debug subsystem blocks. Within the tool, design engineers can specify which functional monitors they want and manage the parameters and interfaces. The tool automatically adds the communication infrastructure.&lt;/li&gt; 
&lt;li&gt;It automates the creation of necessary testbenches for the generated subsystem.&lt;/li&gt; 
&lt;li&gt;It provides industry-standard IP-XACT for clear and consistent register descriptions, streamlining integration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;/li&gt; 
&lt;li&gt;&lt;strong&gt;Seamless UVM integration with UltraSight verification IP &lt;/strong&gt; 
&lt;ul&gt; 
&lt;li&gt;To ensure smooth integration into your existing verification flows and significantly accelerate verification efforts, UltraSight provides a robust, IEEE 1800.2 compliant, UVM-based Verification IP. This VIP serves as a comprehensive test environment, complete with sequence libraries and example integration tests, enabling verification engineers to carry out thorough system-level verification of the Embedded Analytics IP deployed within an SoC.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;/li&gt; 
&lt;li&gt;&lt;strong&gt;Host Suite software for UltraSight configuration, data capture, and real-time insights &lt;/strong&gt; 
&lt;ul&gt; 
&lt;li&gt;For real-time control and data access, Host Suite offers a collection of powerful C++ and Python APIs, and processor run control applications that interface with GDB and OpenOCD. These allow software engineers to configure functional monitoring at run-time and obtain critical monitoring data during software execution.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Efficient monitoring and industry-standard compatibility&lt;/h2&gt; 
&lt;p&gt;Gathering functional monitoring data can quickly become an overwhelming stream. The sheer volume of data can overwhelm the infrastructure to be transferred off-chip, let alone burdening the system bandwidth and adding complexity in post-processing.&lt;/p&gt; 
&lt;p&gt;To address this, Tessent UltraSight provides the following intelligent features:&lt;/p&gt; 
&lt;ul&gt; 
&lt;li&gt;&lt;strong&gt;Advanced filtering and cross-triggering:&lt;/strong&gt;Instead of capturing everything, UltraSight allows you to decide what data to collect and when. You can capture only what’s needed around a specific event, significantly mitigating high bandwidth issues. This smart approach means you focus on relevant data, not noise.&lt;/li&gt; 
&lt;li&gt;&lt;strong&gt;Statistical gathering:&lt;/strong&gt; Beyond raw traces, UltraSight can collect valuable statistics on functional behavior, offering high-level insights without the massive data footprint.&lt;/li&gt; 
&lt;li&gt;&lt;strong&gt;Industry-standard interfaces:&lt;/strong&gt; UltraSight ensures compatibility with common transport mechanisms like JTAG (IEEE 1149.1), cJTAG (IEEE 1149.7), USB, and AMBA AXI, fitting effortlessly into your existing debug environment.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;Tessent UltraSight isn’t just a collection of hardware monitors, it’s a comprehensive solution that combines embedded hardware IP with powerful host software to provide deep visibility into complex SoCs, supporting the development and optimization of high-performance embedded software.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://i0.wp.com/semiengineering.com/wp-content/uploads/2025/12/Siemens_system-wide-debug-for-complex-SoCs-fig1.webp?resize=1280%2C628&amp;amp;ssl=1&quot; alt=&quot;&quot; width=&quot;1280&quot; height=&quot;628&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Key advantages of Tessent UltraSight include:&lt;/p&gt; 
&lt;ul&gt; 
&lt;li&gt;&lt;strong&gt;Comprehensive system-wide functional monitoring:&lt;/strong&gt; Gain deep insights across your entire SoC.&lt;/li&gt; 
&lt;li&gt;&lt;strong&gt;End-to-end solution:&lt;/strong&gt; A complete package consisting of embedded IP and host software for a seamless experience.&lt;/li&gt; 
&lt;li&gt;&lt;strong&gt;Efficient debugging capabilities:&lt;/strong&gt; Smart filtering, statistical gathering, and cross-triggering mitigate bandwidth issues and help you focus on critical events.&lt;/li&gt; 
&lt;li&gt;&lt;strong&gt;Industry-standard integration:&lt;/strong&gt; Compatible with common interfaces and tools, ensuring easy adoption into your existing workflows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Siemens’ deep expertise and leadership in RISC-V debug and trace solutions are reflected in Tessent UltraSight’s capabilities.&lt;/p&gt; 
&lt;p&gt;For more information on Tessent UltraSight and other advanced embedded analytical solutions from Siemens EDA, please visit &lt;a href=&quot;http://www.siemens.com/Tessent-Embedded-Analytics&quot;&gt;www.siemens.com/Tessent-Embedded-Analytics&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The post &lt;a href=&quot;https://semiengineering.com/beyond-the-core-tackling-system-wide-debugging-for-complex-socs/&quot;&gt;Beyond The Core: Tackling System-Wide Debugging For Complex SoCs&lt;/a&gt; appeared first on &lt;a href=&quot;https://semiengineering.com&quot;&gt;Semiconductor Engineering&lt;/a&gt;.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://semiengineering.com/beyond-the-core-tackling-system-wide-debugging-for-complex-socs/</guid>
      <pubDate>Tue, 09 Dec 2025 08:37:47 GMT</pubDate>
    </item>

    <item>
      <title>Enhancing CMP Process Control with Intelligent Line Monitoring &amp; Integrated Metrology</title>
      <link>https://semiengineering.com/enhancing-cmp-process-control-with-intelligent-line-monitoring-integrated-metrology/</link>
      <description>&lt;p&gt;&lt;img src=&quot;https://semiengineering.com/wp-content/uploads/AdobeStock_517438105-04-16-24.jpeg?fit=1200%2C800&amp;amp;ssl=1&quot; alt=&quot;AdobeStock_517438105-04-16-24.jpeg?fit=1&quot;&gt;&lt;/p&gt;&lt;p&gt;New logic transistor designs, 3D NAND stacking, and DRAM integration introduce more CMP layers and tighter process windows. Traditional metrology approaches struggle to keep pace, especially with the need for high sampling rates, multiple control zones, and improved signal-to-noise ratios. Onto Innovation’s Intelligent Line Monitoring &amp;amp; Control with Integrated Metrology offers a new approach to CMP process control. By leveraging data feedforward from standalone metrology tools to integrated metrology systems, this solution enables real-time, AI-driven process optimization.&lt;/p&gt;  
&lt;p&gt;Read &lt;a href=&quot;https://ontoinnovation.com/resources/enhancing-cmp-process-control-with-intelligent-line-monitoring-integrated-metrology/&quot;&gt;more here&lt;/a&gt;.&lt;/p&gt;  
&lt;p&gt;The post &lt;a href=&quot;https://semiengineering.com/enhancing-cmp-process-control-with-intelligent-line-monitoring-integrated-metrology/&quot;&gt;Enhancing CMP Process Control with Intelligent Line Monitoring &amp;amp; Integrated Metrology&lt;/a&gt; appeared first on &lt;a href=&quot;https://semiengineering.com&quot;&gt;Semiconductor Engineering&lt;/a&gt;.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://semiengineering.com/enhancing-cmp-process-control-with-intelligent-line-monitoring-integrated-metrology/</guid>
      <pubDate>Tue, 09 Dec 2025 08:37:47 GMT</pubDate>
    </item>
  </channel>
</rss>